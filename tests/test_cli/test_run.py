"""Tests for the run command and input parsing.

This module tests:
- Input flag parsing (--input name=value)
- Type coercion for input values
- InputCollector for --input.name=value patterns
- Run command execution with mock provider
"""

from __future__ import annotations

from pathlib import Path
from unittest.mock import patch

import pytest
from typer.testing import CliRunner

from copilot_conductor.cli.app import app
from copilot_conductor.cli.run import (
    InputCollector,
    coerce_value,
    parse_input_flags,
)

runner = CliRunner()


class TestCoerceValue:
    """Tests for the coerce_value function."""

    def test_coerce_true(self) -> None:
        """Test coercing 'true' to boolean."""
        assert coerce_value("true") is True
        assert coerce_value("True") is True
        assert coerce_value("TRUE") is True

    def test_coerce_false(self) -> None:
        """Test coercing 'false' to boolean."""
        assert coerce_value("false") is False
        assert coerce_value("False") is False
        assert coerce_value("FALSE") is False

    def test_coerce_null(self) -> None:
        """Test coercing 'null' to None."""
        assert coerce_value("null") is None
        assert coerce_value("Null") is None
        assert coerce_value("NULL") is None

    def test_coerce_integer(self) -> None:
        """Test coercing integer strings."""
        assert coerce_value("42") == 42
        assert coerce_value("-10") == -10
        assert coerce_value("0") == 0

    def test_coerce_float(self) -> None:
        """Test coercing float strings."""
        assert coerce_value("3.14") == 3.14
        assert coerce_value("-2.5") == -2.5
        assert coerce_value("0.0") == 0.0

    def test_coerce_json_array(self) -> None:
        """Test coercing JSON array strings."""
        result = coerce_value('[1, 2, 3]')
        assert result == [1, 2, 3]

    def test_coerce_json_object(self) -> None:
        """Test coercing JSON object strings."""
        result = coerce_value('{"key": "value"}')
        assert result == {"key": "value"}

    def test_coerce_invalid_json_returns_string(self) -> None:
        """Test that invalid JSON returns the original string."""
        result = coerce_value('[not valid json')
        assert result == '[not valid json'

    def test_coerce_string(self) -> None:
        """Test that regular strings are returned unchanged."""
        assert coerce_value("hello") == "hello"
        assert coerce_value("Hello World!") == "Hello World!"
        assert coerce_value("") == ""


class TestParseInputFlags:
    """Tests for parse_input_flags function."""

    def test_parse_single_input(self) -> None:
        """Test parsing a single input."""
        result = parse_input_flags(["name=value"])
        assert result == {"name": "value"}

    def test_parse_multiple_inputs(self) -> None:
        """Test parsing multiple inputs."""
        result = parse_input_flags(["name=Alice", "age=30", "active=true"])
        assert result == {"name": "Alice", "age": 30, "active": True}

    def test_parse_value_with_equals(self) -> None:
        """Test parsing value containing equals sign."""
        result = parse_input_flags(["equation=a=b+c"])
        assert result == {"equation": "a=b+c"}

    def test_parse_empty_value(self) -> None:
        """Test parsing empty value."""
        result = parse_input_flags(["empty="])
        assert result == {"empty": ""}

    def test_parse_json_value(self) -> None:
        """Test parsing JSON value."""
        result = parse_input_flags(['data={"key": "value"}'])
        assert result == {"data": {"key": "value"}}

    def test_parse_missing_equals_raises(self) -> None:
        """Test that missing equals raises BadParameter."""
        import typer
        with pytest.raises(typer.BadParameter, match="Invalid input format"):
            parse_input_flags(["invalid"])

    def test_parse_empty_name_raises(self) -> None:
        """Test that empty name raises BadParameter."""
        import typer
        with pytest.raises(typer.BadParameter, match="Empty input name"):
            parse_input_flags(["=value"])


class TestInputCollector:
    """Tests for InputCollector class."""

    def test_extract_input_dot_pattern(self) -> None:
        """Test extracting --input.name=value pattern."""
        args = ["run", "workflow.yaml", "--input.question=Hello"]
        result = InputCollector.extract_from_args(args)
        assert result == {"question": "Hello"}

    def test_extract_multiple_inputs(self) -> None:
        """Test extracting multiple inputs."""
        args = [
            "run", "workflow.yaml",
            "--input.name=Alice",
            "--input.age=30",
            "--input.active=true",
        ]
        result = InputCollector.extract_from_args(args)
        assert result == {"name": "Alice", "age": 30, "active": True}

    def test_extract_with_type_coercion(self) -> None:
        """Test that values are type coerced."""
        args = [
            "--input.count=42",
            "--input.ratio=3.14",
            "--input.enabled=true",
            "--input.data=[1, 2, 3]",
        ]
        result = InputCollector.extract_from_args(args)
        assert result == {
            "count": 42,
            "ratio": 3.14,
            "enabled": True,
            "data": [1, 2, 3],
        }

    def test_extract_ignores_other_flags(self) -> None:
        """Test that non-input flags are ignored."""
        args = [
            "run", "workflow.yaml",
            "--provider", "copilot",
            "--input.name=Alice",
            "--verbose",
        ]
        result = InputCollector.extract_from_args(args)
        assert result == {"name": "Alice"}

    def test_extract_empty_args(self) -> None:
        """Test with empty args."""
        result = InputCollector.extract_from_args([])
        assert result == {}


class TestRunCommand:
    """Tests for the run command."""

    def test_run_command_help(self) -> None:
        """Test that run --help works."""
        result = runner.invoke(app, ["run", "--help"])
        assert result.exit_code == 0
        assert "Run a workflow from a YAML file" in result.output

    def test_run_command_missing_file(self) -> None:
        """Test that missing file produces error."""
        result = runner.invoke(app, ["run", "nonexistent.yaml"])
        # Should fail because file doesn't exist
        assert result.exit_code != 0

    def test_run_command_with_inputs(self, tmp_path: Path) -> None:
        """Test run command with input flags and mock provider."""
        # Create a simple workflow file
        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: test-workflow
  entry_point: greeter

agents:
  - name: greeter
    model: gpt-4
    prompt: "Say hello to {{ workflow.input.name }}"
    output:
      greeting:
        type: string
    routes:
      - to: $end

output:
  message: "{{ greeter.output.greeting }}"
""")

        # Mock the run_workflow_async function
        with patch("copilot_conductor.cli.run.run_workflow_async") as mock_run:
            mock_run.return_value = {"message": "Hello, World!"}

            runner.invoke(app, [
                "run", str(workflow_file),
                "-i", "name=World",
            ])

            # Check the mock was called
            assert mock_run.called
            call_args = mock_run.call_args

            # Verify inputs were passed
            assert call_args[0][1] == {"name": "World"}

    def test_run_command_with_provider_override(self, tmp_path: Path) -> None:
        """Test run command with provider override."""
        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: test-workflow
  entry_point: agent1

agents:
  - name: agent1
    model: gpt-4
    prompt: "Hello"
    routes:
      - to: $end

output:
  result: "done"
""")

        with patch("copilot_conductor.cli.run.run_workflow_async") as mock_run:
            mock_run.return_value = {"result": "done"}

            runner.invoke(app, [
                "run", str(workflow_file),
                "--provider", "copilot",
            ])

            # Verify provider was passed
            assert mock_run.called
            call_args = mock_run.call_args
            assert call_args[0][2] == "copilot"

    def test_run_command_json_output(self, tmp_path: Path) -> None:
        """Test that output is valid JSON."""
        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: test-workflow
  entry_point: agent1

agents:
  - name: agent1
    prompt: "Hello"
    routes:
      - to: $end

output:
  greeting: "Hello"
""")

        with patch("copilot_conductor.cli.run.run_workflow_async") as mock_run:
            mock_run.return_value = {"greeting": "Hello, World!"}

            result = runner.invoke(app, ["run", str(workflow_file)])

            # Output should be valid JSON
            # The output may have ANSI codes from Rich, so we just check it contains JSON
            assert "Hello, World!" in result.output or result.exit_code == 0


class TestVersionFlag:
    """Tests for the --version flag."""

    def test_version_flag(self) -> None:
        """Test --version shows version."""
        result = runner.invoke(app, ["--version"])
        assert result.exit_code == 0
        assert "Copilot Conductor v" in result.output

    def test_version_short_flag(self) -> None:
        """Test -v shows version."""
        result = runner.invoke(app, ["-v"])
        assert result.exit_code == 0
        assert "Copilot Conductor v" in result.output


class TestHelpFlag:
    """Tests for the --help flag."""

    def test_help_flag(self) -> None:
        """Test --help shows help."""
        result = runner.invoke(app, ["--help"])
        assert result.exit_code == 0
        assert "Copilot Conductor" in result.output
        assert "run" in result.output

    def test_no_args_shows_help(self) -> None:
        """Test running with no args shows help."""
        result = runner.invoke(app, [])
        # Typer with no_args_is_help=True shows help but returns exit code 2
        # when no command is provided (this is expected Typer behavior)
        assert "Usage" in result.output
        assert "run" in result.output


class TestDryRunMode:
    """Tests for the --dry-run flag."""

    def test_dry_run_help(self) -> None:
        """Test --dry-run is documented in help."""
        result = runner.invoke(app, ["run", "--help"])
        assert result.exit_code == 0
        assert "--dry-run" in result.output

    def test_dry_run_simple_workflow(self, tmp_path: Path) -> None:
        """Test dry-run with a simple linear workflow."""
        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: simple-test
  entry_point: agent1

agents:
  - name: agent1
    model: gpt-4
    prompt: "Hello"
    routes:
      - to: $end

output:
  result: "{{ agent1.output }}"
""")

        result = runner.invoke(app, ["run", str(workflow_file), "--dry-run"])

        # Should succeed
        assert result.exit_code == 0
        # Should show execution plan header
        assert "Execution Plan" in result.output or "Dry Run" in result.output
        # Should show agent name
        assert "agent1" in result.output
        # Should show the model
        assert "gpt-4" in result.output

    def test_dry_run_multi_agent_workflow(self, tmp_path: Path) -> None:
        """Test dry-run with multiple agents."""
        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: multi-agent-test
  entry_point: planner
  limits:
    max_iterations: 15
    timeout_seconds: 120

agents:
  - name: planner
    model: gpt-4
    prompt: "Plan something"
    routes:
      - to: executor

  - name: executor
    model: claude-sonnet-4
    prompt: "Execute the plan"
    routes:
      - to: $end

output:
  result: "{{ executor.output }}"
""")

        result = runner.invoke(app, ["run", str(workflow_file), "--dry-run"])

        # Should succeed
        assert result.exit_code == 0
        # Should show both agents
        assert "planner" in result.output
        assert "executor" in result.output
        # Should show both models
        assert "gpt-4" in result.output
        assert "claude-sonnet-4" in result.output or "claude" in result.output

    def test_dry_run_conditional_routing(self, tmp_path: Path) -> None:
        """Test dry-run with conditional routes."""
        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: conditional-test
  entry_point: checker

agents:
  - name: checker
    model: gpt-4
    prompt: "Check condition"
    routes:
      - to: success_handler
        when: "{{ output.success }}"
      - to: failure_handler
        when: "{{ not output.success }}"

  - name: success_handler
    model: gpt-4
    prompt: "Handle success"
    routes:
      - to: $end

  - name: failure_handler
    model: gpt-4
    prompt: "Handle failure"
    routes:
      - to: $end

output:
  result: "done"
""")

        result = runner.invoke(app, ["run", str(workflow_file), "--dry-run"])

        # Should succeed
        assert result.exit_code == 0
        # Should show all agents (may be truncated in table display)
        assert "checker" in result.output
        # Rich may truncate agent names with ellipsis, so check for prefix
        assert "success_hand" in result.output
        assert "failure_hand" in result.output

    def test_dry_run_loop_workflow(self, tmp_path: Path) -> None:
        """Test dry-run with loop-back pattern."""
        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: loop-test
  entry_point: generator

agents:
  - name: generator
    model: gpt-4
    prompt: "Generate something"
    routes:
      - to: reviewer

  - name: reviewer
    model: gpt-4
    prompt: "Review the generation"
    routes:
      - to: $end
        when: "{{ output.approved }}"
      - to: generator

output:
  result: "{{ generator.output }}"
""")

        result = runner.invoke(app, ["run", str(workflow_file), "--dry-run"])

        # Should succeed
        assert result.exit_code == 0
        # Should show agents
        assert "generator" in result.output
        assert "reviewer" in result.output
        # Should indicate loop (the "loop target" marker)
        assert "loop" in result.output.lower() or "target" in result.output.lower()

    def test_dry_run_human_gate_workflow(self, tmp_path: Path) -> None:
        """Test dry-run with human gate."""
        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: gate-test
  entry_point: generator

agents:
  - name: generator
    model: gpt-4
    prompt: "Generate content"
    routes:
      - to: reviewer

  - name: reviewer
    type: human_gate
    prompt: "Review the content"
    options:
      - label: Approve
        value: approved
        route: $end
      - label: Reject
        value: rejected
        route: generator

output:
  result: "{{ generator.output }}"
""")

        result = runner.invoke(app, ["run", str(workflow_file), "--dry-run"])

        # Should succeed
        assert result.exit_code == 0
        # Should show agents
        assert "generator" in result.output
        assert "reviewer" in result.output
        # Should show human_gate type
        assert "human_gate" in result.output

    def test_dry_run_does_not_execute(self, tmp_path: Path) -> None:
        """Test that dry-run doesn't actually execute the workflow."""
        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: test-workflow
  entry_point: agent1

agents:
  - name: agent1
    model: gpt-4
    prompt: "Hello"
    routes:
      - to: $end

output:
  result: "{{ agent1.output }}"
""")

        # Mock the run_workflow_async to ensure it's never called
        with patch("copilot_conductor.cli.run.run_workflow_async") as mock_run:
            result = runner.invoke(app, ["run", str(workflow_file), "--dry-run"])

            # Should succeed
            assert result.exit_code == 0
            # run_workflow_async should NOT be called
            assert not mock_run.called

    def test_dry_run_invalid_workflow(self, tmp_path: Path) -> None:
        """Test dry-run with invalid workflow file."""
        workflow_file = tmp_path / "invalid.yaml"
        workflow_file.write_text("""\
workflow:
  name: invalid-test
  entry_point: nonexistent_agent

agents:
  - name: agent1
    model: gpt-4
    prompt: "Hello"

output: {}
""")

        result = runner.invoke(app, ["run", str(workflow_file), "--dry-run"])

        # Should fail with validation error
        assert result.exit_code != 0

    def test_dry_run_parallel_workflow(self, tmp_path: Path) -> None:
        """Test dry-run output with parallel groups."""
        workflow_file = tmp_path / "parallel.yaml"
        workflow_file.write_text("""\
workflow:
  name: parallel-test
  entry_point: coordinator

agents:
  - name: coordinator
    model: gpt-4
    prompt: "Start parallel tasks"
    routes:
      - to: parallel_research

  - name: research_a
    model: gpt-4
    prompt: "Research A"

  - name: research_b
    model: gpt-4
    prompt: "Research B"

  - name: synthesizer
    model: gpt-4
    prompt: "Synthesize results"
    routes:
      - to: $end

parallel:
  - name: parallel_research
    agents:
      - research_a
      - research_b
    failure_mode: fail_fast
    routes:
      - to: synthesizer

output:
  result: "{{ synthesizer.output }}"
""")

        result = runner.invoke(app, ["run", str(workflow_file), "--dry-run"])

        # Should succeed
        assert result.exit_code == 0
        # Should show parallel group (may be truncated in narrow display)
        assert "parallel_res" in result.output
        # Type column shows truncated "parallel_gr..." for space, so check Type header instead
        assert "Type" in result.output
        # Should show failure mode
        assert "fail_fast" in result.output
        # Should show parallel agents
        assert "research_a" in result.output
        assert "research_b" in result.output
        # Should show parallel stats in summary
        assert "Parallel groups:" in result.output
        assert "Parallel agents:" in result.output

    def test_dry_run_parallel_with_continue_on_error(self, tmp_path: Path) -> None:
        """Test dry-run with continue_on_error failure mode."""
        workflow_file = tmp_path / "parallel_continue.yaml"
        workflow_file.write_text("""\
workflow:
  name: parallel-continue
  entry_point: parallel_validators

agents:
  - name: validator_a
    model: gpt-4
    prompt: "Validate A"

  - name: validator_b
    model: gpt-4
    prompt: "Validate B"

  - name: validator_c
    model: gpt-4
    prompt: "Validate C"

  - name: report
    model: gpt-4
    prompt: "Generate report"
    routes:
      - to: $end

parallel:
  - name: parallel_validators
    agents:
      - validator_a
      - validator_b
      - validator_c
    failure_mode: continue_on_error
    routes:
      - to: report

output:
  result: "{{ report.output }}"
""")

        result = runner.invoke(app, ["run", str(workflow_file), "--dry-run"])

        # Should succeed
        assert result.exit_code == 0
        # Should show continue_on_error mode
        assert "continue_on_error" in result.output
        # Should show all three validators
        assert "validator_a" in result.output
        assert "validator_b" in result.output
        assert "validator_c" in result.output
        # Should show parallel group stats in summary
        assert "Parallel groups:" in result.output


class TestDryRunDisplayFunctions:
    """Tests for dry-run display helper functions."""

    def test_format_routes_empty(self) -> None:
        """Test format_routes with empty list."""
        from copilot_conductor.cli.run import format_routes

        result = format_routes([])
        assert "$end" in result

    def test_format_routes_unconditional(self) -> None:
        """Test format_routes with unconditional route."""
        from copilot_conductor.cli.run import format_routes

        routes = [{"to": "next_agent", "when": None, "is_conditional": False}]
        result = format_routes(routes)
        assert "next_agent" in result
        assert "if" not in result.lower()

    def test_format_routes_conditional(self) -> None:
        """Test format_routes with conditional route."""
        from copilot_conductor.cli.run import format_routes

        routes = [{"to": "next_agent", "when": "output.success", "is_conditional": True}]
        result = format_routes(routes)
        assert "next_agent" in result
        assert "if" in result.lower()

    def test_format_routes_multiple(self) -> None:
        """Test format_routes with multiple routes."""
        from copilot_conductor.cli.run import format_routes

        routes = [
            {"to": "agent_a", "when": "condition1", "is_conditional": True},
            {"to": "agent_b", "when": None, "is_conditional": False},
        ]
        result = format_routes(routes)
        assert "agent_a" in result
        assert "agent_b" in result

    def test_format_routes_long_condition_truncated(self) -> None:
        """Test that long conditions are truncated."""
        from copilot_conductor.cli.run import format_routes

        long_condition = "a" * 100  # Very long condition
        routes = [{"to": "next", "when": long_condition, "is_conditional": True}]
        result = format_routes(routes)
        # Should be truncated
        assert "..." in result


class TestBuildDryRunPlan:
    """Tests for build_dry_run_plan function."""

    def test_build_plan_simple_workflow(self, tmp_path: Path) -> None:
        """Test building execution plan for simple workflow."""
        from copilot_conductor.cli.run import build_dry_run_plan

        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: test-workflow
  entry_point: agent1

agents:
  - name: agent1
    model: gpt-4
    prompt: "Hello"
    routes:
      - to: $end

output:
  result: "done"
""")

        plan = build_dry_run_plan(workflow_file)

        assert plan.workflow_name == "test-workflow"
        assert plan.entry_point == "agent1"
        assert len(plan.steps) == 1
        assert plan.steps[0].agent_name == "agent1"
        assert plan.steps[0].model == "gpt-4"

    def test_build_plan_multi_agent(self, tmp_path: Path) -> None:
        """Test building execution plan with multiple agents."""
        from copilot_conductor.cli.run import build_dry_run_plan

        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: test-workflow
  entry_point: agent1

agents:
  - name: agent1
    model: gpt-4
    prompt: "First"
    routes:
      - to: agent2

  - name: agent2
    model: claude-sonnet-4
    prompt: "Second"
    routes:
      - to: $end

output:
  result: "done"
""")

        plan = build_dry_run_plan(workflow_file)

        assert len(plan.steps) == 2
        assert plan.steps[0].agent_name == "agent1"
        assert plan.steps[1].agent_name == "agent2"

    def test_build_plan_with_limits(self, tmp_path: Path) -> None:
        """Test that limits are captured in the plan."""
        from copilot_conductor.cli.run import build_dry_run_plan

        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: test-workflow
  entry_point: agent1
  limits:
    max_iterations: 25
    timeout_seconds: 300

agents:
  - name: agent1
    prompt: "Hello"
    routes:
      - to: $end

output:
  result: "done"
""")

        plan = build_dry_run_plan(workflow_file)

        assert plan.max_iterations == 25
        assert plan.timeout_seconds == 300

    def test_build_plan_detects_loop(self, tmp_path: Path) -> None:
        """Test that loop targets are detected."""
        from copilot_conductor.cli.run import build_dry_run_plan

        workflow_file = tmp_path / "test.yaml"
        workflow_file.write_text("""\
workflow:
  name: test-workflow
  entry_point: agent1

agents:
  - name: agent1
    model: gpt-4
    prompt: "Generate"
    routes:
      - to: agent2

  - name: agent2
    model: gpt-4
    prompt: "Review"
    routes:
      - to: $end
        when: "{{ output.done }}"
      - to: agent1

output:
  result: "done"
""")

        plan = build_dry_run_plan(workflow_file)

        # agent1 should be marked as a loop target
        agent1_step = next(s for s in plan.steps if s.agent_name == "agent1")
        assert agent1_step.is_loop_target is True
